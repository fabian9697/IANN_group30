{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Task 2: Data Set\n",
    "# Description: An array of possible binary inputs is defined as x. \n",
    "#              Moreover the targets, t, for the logical gates: and, or, not and, not or and exclusive or (xor) are defined.\n",
    "\n",
    "x = np.array([[0,0],[0,1],[1,0],[1,1]]) \n",
    "\n",
    "t_and = np.array([0,0,0,1])\n",
    "t_or = np.array([0,1,1,1])\n",
    "t_nand = np.array([1,1,1,0])\n",
    "t_nor = np.array([1,0,0,0])\n",
    "t_xor = np.array([0,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Perceptron\n",
    "# Description: The class Perceptron describes the behavior of a single neuron and includes:\n",
    "#              @init variables: input_units\n",
    "#              @class variables: -\n",
    "#              @object variables: input_units, alpha, weights, bias\n",
    "#              @functions: forward_step_drive, update\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_units):\n",
    "        self.input_units = input_units\n",
    "        self.alpha = 0.01      #learning rates\n",
    "        self.weights = np.random.randn(self.input_units)\n",
    "        self.bias = np.random.randn(1)\n",
    "        self.data = []\n",
    "    \n",
    "    # Description: Calculates the output of the perceptron depending on the input, bias and weights.\n",
    "    def forward_step_drive(self, data):\n",
    "        self.data = data\n",
    "        drive = self.weights @ self.data + self.bias\n",
    "        return drive\n",
    "    \n",
    "    #Description: Updates the bias and weights depending on the calculated error.\n",
    "    def update(self, delta):\n",
    "        \n",
    "        w_gradient = delta * self.data\n",
    "        self.weights = self.weights - (self.alpha * np.transpose(w_gradient))\n",
    "        \n",
    "        b_gradient = delta * 1\n",
    "        self.bias = self.bias - (self.alpha * b_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Multi-Layer Perceptron\n",
    "# Description: The class MLP describes a multi-layer perceptron with one hiddenlayer including 4 perceptrons\n",
    "#              and one output perceptron and includes:\n",
    "#              @init variables: input_units\n",
    "#              @class variables: -\n",
    "#              @object variables: h1_1, h1_2, h1_3, h1_4, out_1\n",
    "#              @functions: forward_step_out, backprop_step\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_units):\n",
    "        \n",
    "        # definition hidden layer 1\n",
    "        self.h1_1 = {'Perceptron':Perceptron(input_units)}\n",
    "        self.h1_2 = {'Perceptron':Perceptron(input_units)}\n",
    "        self.h1_3 = {'Perceptron':Perceptron(input_units)}\n",
    "        self.h1_4 = {'Perceptron':Perceptron(input_units)}\n",
    "              \n",
    "        # defintion ouput layer\n",
    "        self.out_1 = {'Perceptron':Perceptron(4)}\n",
    "    \n",
    "    # Description: Calculates the output of the multi-layer perceptron (MLP), depending on the input and including perceptrons.\n",
    "    def forward_step_out(self,data):\n",
    "        self.h1_1.update({'drive':self.h1_1['Perceptron'].forward_step_drive(data)})  \n",
    "        self.h1_2.update({'drive':self.h1_2['Perceptron'].forward_step_drive(data)})\n",
    "        self.h1_3.update({'drive':self.h1_3['Perceptron'].forward_step_drive(data)})\n",
    "        self.h1_4.update({'drive':self.h1_4['Perceptron'].forward_step_drive(data)})\n",
    "        \n",
    "        h1_out = [self.sigmoid(self.h1_1['drive']), self.sigmoid(self.h1_2['drive']), self.sigmoid(self.h1_3['drive']), self.sigmoid(self.h1_4['drive'])]\n",
    "        \n",
    "        self.out_1.update({'drive':self.out_1['Perceptron'].forward_step_drive(h1_out)})\n",
    "        \n",
    "        return self.sigmoid(self.out_1['drive'])\n",
    "        \n",
    "    # Description: Updates the weights and bias of all perceptrons in the multi-layer perceptron (MLP).\n",
    "    def backprop_step(self, target):\n",
    "                \n",
    "        # Output Layer     \n",
    "        delta = self.calc_delta(target, self.out_1['drive'])\n",
    "        self.out_1['Perceptron'].update(delta)\n",
    "        \n",
    "        # Hidden Layer\n",
    "        # Perceptron 1\n",
    "        delta = self.calc_delta(target, self.h1_1['drive'])\n",
    "        self.h1_1['Perceptron'].update(delta)\n",
    "        \n",
    "        # Perceptron 2\n",
    "        delta = self.calc_delta(target, self.h1_2['drive'])\n",
    "        self.h1_2['Perceptron'].update(delta)\n",
    "        \n",
    "        # Perceptron 3\n",
    "        delta = self.calc_delta(target, self.h1_3['drive'])\n",
    "        self.h1_3['Perceptron'].update(delta)\n",
    "        \n",
    "        # Perceptron 4\n",
    "        delta = self.calc_delta(target, self.h1_4['drive'])\n",
    "        self.h1_4['Perceptron'].update(delta)\n",
    "        \n",
    " \n",
    "    # Description: Trains the multi-layer perception.\n",
    "    def training_step(self, input, target):      \n",
    "        self.forward_step_out(input)\n",
    "        self.backprop_step(target)\n",
    "\n",
    "    # Description: Calculates the delta, representing the loss\n",
    "    def calc_delta(self, target, drive):\n",
    "        delta = ((target - self.sigmoid(drive))**2) * self.sigmoid_prime(drive)\n",
    "        return delta\n",
    "    \n",
    "    # Description: Represents the activation function of the perceptron.\n",
    "    def sigmoid(self, x): \n",
    "        return (1/(1+np.exp(-x)))\n",
    "    \n",
    "    # Description: Represents the derivative of the activation function of the perceptron.\n",
    "    def sigmoid_prime(self, x):\n",
    "        return (self.sigmoid(x) * (1 - self.sigmoid(x))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target from above\n",
    "t = t_and\n",
    "\n",
    "# Create the multi-layer perceptron\n",
    "my_mlp = MLP(2)\n",
    "\n",
    "\n",
    "# Define Training Loop\n",
    "\n",
    "steps = []\n",
    "accuracies = []\n",
    "\n",
    "for i in range(10000):\n",
    "    steps.append(i)\n",
    "    \n",
    "    #take a random sample from data, get corresponding t\n",
    "    index = np.random.randint(4) #random number from 1, 2, 3 or 4\n",
    "    data_input = x[index]   #random variablen paar aus x\n",
    "    target = t[index]  #richtige Antwort zum Input\n",
    "\n",
    "    my_mlp.training_step(data_input, target)\n",
    "    \n",
    "    # Performance of multi-layer perceptron\n",
    "    accuracy_sum = 0\n",
    "    for k in range(4):\n",
    "        if my_mlp.forward_step_out(x[k]) > 0.5:\n",
    "            output = 1\n",
    "        else:\n",
    "            output = 0\n",
    "        accuracy_sum = accuracy_sum + int(output == t[k])\n",
    "\n",
    "    accuracies.append(accuracy_sum/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize training\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(steps, accuracies)\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlim([-1,5000])\n",
    "plt.ylim([-0.1, 1.2])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
