{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "homework04_group30.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBZVkfNzIk-I"
      },
      "source": [
        "**Initialization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA9JiLNdIWW2"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sger5hN76j7D"
      },
      "source": [
        "**Task 1: Data set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6F73u6zIo8W"
      },
      "source": [
        "# Input pipeline\n",
        "batch_size = 64\n",
        "\n",
        "raw_dataset = tfds.load('Malaria', split='train', as_supervised = True)    # Name the split parameter to get a tf.dataset with tuples. Otherwise one gets a dictionary.\n",
        "\n",
        "# Resizes the images to a quadratic shape of 300x300 pixels. \n",
        "padded_images = raw_dataset.map(lambda inp, tar: tf.image.resize(inp, [394, 394]))\n",
        "\n",
        "# Normalizes/ Standardizes the images with a mean of zero and a standard deviation of 1.\n",
        "padded_images = padded_images.map(lambda img: tf.image.per_image_standardization(img)) \n",
        "\n",
        "# One-hot encoded targets\n",
        "one_hot_targets = raw_dataset.map(lambda inp, tar: tf.one_hot(tar, 2))\n",
        "\n",
        "\n",
        "# Seperates the inputs (images) and targets into training and test data.\n",
        "splitting_limit_train_test_data = 22000\n",
        "training_dataset_inputs = padded_images.take(splitting_limit_train_test_data)      # new datasets with all items up to the splitting limit\n",
        "training_dataset_targets = one_hot_targets.take(splitting_limit_train_test_data)   \n",
        "\n",
        "test_dataset_inputs = padded_images.skip(splitting_limit_train_test_data)          # new datasets with all items from the splitting limit\n",
        "test_dataset_targets = one_hot_targets.skip(splitting_limit_train_test_data)       \n",
        "\n",
        "\n",
        "# Zips together, shuffles, batches and prefetches the training and test datasets.\n",
        "training_dataset = tf.data.Dataset.zip((training_dataset_inputs, training_dataset_targets))\n",
        "training_dataset = training_dataset.batch(batch_size)\n",
        "training_dataset = training_dataset.shuffle(buffer_size = batch_size)\n",
        "training_dataset = training_dataset.prefetch(32)\n",
        "\n",
        "test_dataset = tf.data.Dataset.zip((test_dataset_inputs, test_dataset_targets))\n",
        "test_dataset = test_dataset.batch(batch_size)\n",
        "test_dataset = test_dataset.shuffle(buffer_size = batch_size)\n",
        "test_dataset = test_dataset.prefetch(32)   "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWMxHuz-AExR"
      },
      "source": [
        "#for img in padded_images:\n",
        "#  plt.imshow(img.numpy())\n",
        "#  plt.show()\n",
        "\n",
        "#for inp, tar in training_dataset:\n",
        "#  print(tar[0].numpy()) \n",
        "#  plt.imshow(inp[0].numpy())\n",
        "#  plt.show()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sA0V7i9-gQb"
      },
      "source": [
        "**Task 2: Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8gMfRtH-lB_"
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "# Description: The class Model describes a convolutional neural network with a feature extractor and \n",
        "#              a classifier.\n",
        "#              @class variables: -\n",
        "#              @object variables: conv_1, max_pool_1, conv_2, max_pool_2, conv_3, max_pool_3, conv_4, \n",
        "#                                 max_pool_4, global_pool, output\n",
        "#              @functions: call\n",
        "class Model(Model): \n",
        "  \n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # Feature extractor\n",
        "        self.conv_1 = tf.keras.layers.Conv2D(filters=100, kernel_size=3, activation=tf.keras.activations.relu, input_shape=(394, 394, 3))\n",
        "        self.max_pool_1 = tf.keras.layers.MaxPool2D()\n",
        "        self.conv_2 = tf.keras.layers.Conv2D(filters=100, kernel_size=5, activation=tf.keras.activations.relu)\n",
        "        self.max_pool_2 = tf.keras.layers.MaxPool2D()\n",
        "        self.conv_3 = tf.keras.layers.Conv2D(filters=100, kernel_size=5, activation=tf.keras.activations.relu)\n",
        "        self.max_pool_3 = tf.keras.layers.MaxPool2D()\n",
        "        self.conv_4 = tf.keras.layers.Conv2D(filters=100, kernel_size=3, activation=tf.keras.activations.relu)\n",
        "        self.max_pool_4 = tf.keras.layers.MaxPool2D()\n",
        "\n",
        "        # Classifier\n",
        "        self.global_pool = tf.keras.layers.GlobalAveragePooling2D()\n",
        "        self.output_layer = tf.keras.layers.Dense(2, activation=tf.keras.activations.softmax)\n",
        "\n",
        "\n",
        "    # Description: This function conducts one forward-step of the model. \n",
        "    #              A python decorator (@tf.function) is used to bundle multiple computations into one computational graph.\n",
        "    #              @parameters: (input) x\n",
        "    #              @returns: (prediction) x\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        x = self.conv_1(x)\n",
        "        x = self.max_pool_1(x)\n",
        "        x = self.conv_2(x)\n",
        "        x = self.max_pool_2(x)\n",
        "        x = self.conv_3(x)\n",
        "        x = self.max_pool_3(x)\n",
        "        x = self.conv_4(x)\n",
        "        x = self.max_pool_4(x)\n",
        "\n",
        "        x = self.global_pool(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vIQGZNoUxNe"
      },
      "source": [
        "**Task 3: Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoJMeeq0U1N0"
      },
      "source": [
        "# Description: This function trains an object of the class Model. It conducts a forward-step and the backpropagation \n",
        "#              throughout the network. Additionally, it determines the average training loss and accuracy.\n",
        "#              @parameters: model, training_data, loss_fn, optimizer\n",
        "#              @returns: training_loss, training_accuracy\n",
        "def training_step(model, training_data, loss_fn, optimizer):\n",
        "  training_losses = []\n",
        "  training_accuracies = []\n",
        "\n",
        "  for (input, target) in training_data:\n",
        "    with tf.GradientTape() as tape:\n",
        "      prediction = model(input)\n",
        "      current_training_loss = loss_fn(target, prediction)\n",
        "      gradients = tape.gradient(current_training_loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    training_losses.append(current_training_loss.numpy())\n",
        "\n",
        "    current_training_accuracy = np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
        "    training_accuracies.append(np.mean(current_training_accuracy))   \n",
        "  \n",
        "  training_loss = np.mean(training_losses)\n",
        "  training_accuracy = np.mean(training_accuracies)\n",
        "  return training_loss, training_accuracy\n",
        "\n",
        "\n",
        "# Description: This function determines the average test loss and accuracy of an object of the class Model.\n",
        "#              @parameters: model, test_data, loss_fn\n",
        "#              @returns: test_loss, test_accuracy\n",
        "def test(model, test_data, loss_fn):\n",
        "  test_losses = []\n",
        "  test_accuracies = []\n",
        "\n",
        "  for (input, target) in test_data:\n",
        "    prediction = model(input)\n",
        "    \n",
        "    current_test_loss = loss_fn(target, prediction)\n",
        "    test_losses.append(current_test_loss.numpy())\n",
        "\n",
        "    current_test_accuracy = np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
        "    test_accuracies.append(np.mean(current_test_accuracy))   \n",
        "    \n",
        "  test_loss = np.mean(test_losses)\n",
        "  test_accuracy = np.mean(test_accuracies)\n",
        "  return test_loss, test_accuracy"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrprsVmdVSor",
        "outputId": "f031311e-a6bb-417a-ec85-a59005d77011"
      },
      "source": [
        "# Description: This part creates an object of the class Model called model and executes the training and testing of the model in the training and test loop. The training \n",
        "#              takes place over an amount of epochs (n_epochs) with a predefined learning rate. The loss function defines the kind of loss-calculation. The optimizer \n",
        "#              is needed to adjust the gradients in the training steps. Moreover, the data for the visualization of the training and test progress is collected.\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = Model()\n",
        "n_epochs = 10\n",
        "learning_rate = 0.00001\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, amsgrad=True)    # Adam = Adaptive Moment Estimation\n",
        "\n",
        "training_losses = []\n",
        "training_accuracies = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "# Training and test loop\n",
        "for epoch in range(n_epochs):\n",
        "    print('Epoch ' + str(epoch))\n",
        "\n",
        "    training_loss, training_accuracy = training_step(model, training_dataset, loss_fn, optimizer)\n",
        "    training_losses.append(training_loss)\n",
        "    training_accuracies.append(training_accuracy)\n",
        "\n",
        "    test_loss, test_accuracy = test(model, test_dataset, loss_fn)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBGANjYnWQoT"
      },
      "source": [
        "**Task 4: Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h86YtTxWWh-"
      },
      "source": [
        "# Description: Figure 1 shows the loss for each epoch during the training and testing of the model.\n",
        "#              Figure 2 shows the accuracy for each epoch during the training and testing of the model.\n",
        "plt.figure()\n",
        "line1, = plt.plot(training_losses)\n",
        "line2, = plt.plot(test_losses)\n",
        "plt.xlabel(\"Training steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend((line1, line2),(\"Training\", \"Test\"))\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "line1, = plt.plot(training_accuracies)\n",
        "line2, = plt.plot(test_accuracies)\n",
        "plt.grid()\n",
        "plt.ylim(0.5, 1)\n",
        "plt.xlabel(\"Training steps\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend((line1, line2),(\"Training\", \"Test\"))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}