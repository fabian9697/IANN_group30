%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Stylish Article
% LaTeX Template
% Version 2.2 (2020-10-22)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Mathias Legrand (legrand.mathias@gmail.com) 
% With extensive modifications by:
% Vel (vel@latextemplates.com)
% Jacqueline Näther (jnaether@uni-osnabrueck.com)
% Fabian Imkenberg (fimkenberg@uni-osnabrueck.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[fleqn,10pt]{SelfArx} % Document font size and equations flushed left

\usepackage[english]{babel} % Specify a different language here - english by default

\usepackage{lipsum} % Required to insert dummy text. To be removed otherwise

\usepackage[nolist]{acronym}	% [nolist]: Acronym list should not be print out.

\usepackage{listings}	% Required to insert lstlisting environment for code.

\usepackage{makecell}	% For manual line breaks in tables

%----------------------------------------------------------------------------------------
%	COLUMNS
%----------------------------------------------------------------------------------------

\setlength{\columnsep}{0.55cm} % Distance between the two columns of text
\setlength{\fboxrule}{0.75pt} % Width of the border around the abstract

%----------------------------------------------------------------------------------------
%	COLORS
%----------------------------------------------------------------------------------------

\definecolor{color1}{RGB}{172,06,52} % Corporate Design Colour of University of Osnabrueck
\definecolor{color2}{RGB}{207,208,209} % Corporate Design Colour of University of Osnabrueck
\definecolor{color3}{RGB}{251,185,0} % Corporate Design Colour of University of Osnabrueck
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

%----------------------------------------------------------------------------------------
%	LISTING STYLE
%----------------------------------------------------------------------------------------

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

%----------------------------------------------------------------------------------------
%	HYPERLINKS
%----------------------------------------------------------------------------------------

\usepackage{hyperref} % Required for hyperlinks

\hypersetup{
	hidelinks,
	colorlinks,
	breaklinks=true,
	urlcolor=color3,
	citecolor=color1,
	linkcolor=color1,
	bookmarksopen=false,
	pdftitle={Title},
	pdfauthor={Author},
}

%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

\JournalInfo{\includegraphics[width=0.3\linewidth]{logo_uni}}
\Archive{} %!!NEEDED, otherwise \tableofcontent error, is for Additional notes below university logo (e.g. copyright, DOI, review/research article)

\PaperTitle{Reimplementation of a Cycle-Consistent Adversarial Network for unpaired Image-to-Image Translation}

\Authors{Fabian Imkenberg, Jacqueline Naether}

\Keywords{CycleGAN --- Unpaired Image-to-Image Translation --- Keyword3} 
\newcommand{\keywordname}{Keywords} % Defines the keywords heading name

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Ut purus elit, vestibulum ut, placerat ac, adipiscing vitae, felis. Curabitur dictum gravida mauris. Nam arcu libero, nonummy eget, consectetuer id, vulputate a, magna. Donec vehicula augue eu neque. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris ut leo. Cras viverra metus rhoncus sem. Nulla et lectus vestibulum urna fringilla ultrices. Phasellus eu tellus sit amet tortor gravida placerat. Integer sapien est, iaculis in, pretium quis, viverra ac, nunc. Praesent eget sem vel leo ultrices bibendum. Aenean faucibus. Morbi dolor nulla, malesuada eu, pulvinar at, mollis ac, nulla. Curabitur auctor semper nulla. Donec varius orci eget risus. Duis nibh mi, congue eu, accumsan eleifend, sagittis quis, diam. Duis eget orci sit amet orci dignissim rutrum.}

%----------------------------------------------------------------------------------------

\begin{document}
\maketitle
\tableofcontents
\thispagestyle{empty} % Removes page numbering from the first page

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section*{Introduction}
\addcontentsline{toc}{section}{Introduction}
As the basis of our project, we studied the paper \textit{Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks} from the \ac{BAIR} laboratory. The aim of the paper is to change different information in images. The developed network creates a new generated image based on an existing input image. Three different types of generated images are presented in this paper. A translation of a photograph to a painting by Monet, zebras to horses, and summer to winter and vice versa. The paper complements what has been learned in the module \ac{IANNWTF} by two essential building blocks, which basically depend on each other.~\cite{image-to-image-ccan}

Until now, training data was mostly available in input-output pairs. This means that in the example translation summer to winter, for each photo of a location in summer there also exists a photo of the same location in winter. Thus, the network previously learned translation using complete individual examples. In the case of the paper, however, a network is developed that is trained based on two domains. As training input, the network is fed a domain of, for example, summer images. The training output then contains a domain of winter images. The network learns the different properties of summer and winter based on these image domains.~\cite{image-to-image-ccan}

As a second building block, the simple \ac{GAN} as presented in \ac{IANNWTF} is extended into a so-called Cycle-\ac{GAN}. The name is based on the use of the Cycle Consistency Loss in the training phase. We reimplemented and tested such a Cycle-\ac{GAN} based on the paper.~\cite{image-to-image-ccan}

%------------------------------------------------

\section{Methods}

As a transfer performance, our goal is not to teach our network the same translation as described in the paper.  Therefore, we decided to optimize our network to perform a translation from apple images to orange images and vice versa. For this, we had to select a suitable dataset, as well as to work out the methodological basics of the network architecture from the paper. In the following, these basics will be explained.

\subsection{Architectures}
As the basis of the network architecture, the \ac{GAN} already known through the \ac{IANNWTF} module is extended to the Cycle-\ac{GAN} known through the paper \cite{image-to-image-ccan}.

\paragraph{\acl{GAN}s} basically exist out of two neural networks, which more or less battle against each other, the \textit{Generator} and \textit{Discriminator}. The Generator takes the input and generates a fake image out of it, while the Discriminator tries to discriminate between generated and true images. Both parts of the network are trained individually from each other. The loss used in training is the adveresarial loss.~\cite{Introduction-to-Cycle-GANs, GAN-Courseware}

\paragraph{Cycle-\ac{GAN}s} extend this concept, in terms of unpaired image-to-image translation. As mentioned in the introduction, the choosen paper uses domains of input \textit{D\textsubscript{x}} and output images \textit{D\textsubscript{y}}. The Generator is trained two main functions. Generating images from domain \textit{D\textsubscript{x}} to images of domain \textit{D\textsubscript{y}} ($G: X \rightarrow Y$) and vice versa ($F: Y \rightarrow X$). This principle is shown in \autoref{fig:mappingFunctions}.~\cite{image-to-image-ccan}

\begin{figure} \centering 
% Using \begin{figure*} makes the figure take up the entire width of the page
	\includegraphics[width=0.8\linewidth]{mappingFunctions}
	\caption{Mapping functions of the Cycle-\ac{GAN}~\cite{image-to-image-ccan}}
	\label{fig:mappingFunctions}
\end{figure}

On the basis of this it becomes clear that a simple back and forward translation can lead to the fact that the back translation does not refer to exactly the original input image, but to another image within the input image domain. This is where the cycle consistency loss comes into play. This one is used in addition to the adversarial loss so that the network learns to return to the original image during a back and forward translation. The cycle consistency losses for the two mapping functions are shown in the \autoref{fig:cycleConsistencyLoss}.~\cite{image-to-image-ccan}

\begin{figure*}[htb] \centering 
% Using \begin{figure*} makes the figure take up the entire width of the page
	\includegraphics[width=\linewidth]{cycleConsistencyLoss}
	\caption{Cycle Consistency Losses in the back and forward translation~\cite{image-to-image-ccan}}
	\label{fig:cycleConsistencyLoss}
\end{figure*}


\subsection{Formulation}
In order to better understand the function and subsequent implementation of the losses, the mathematical background will be discussed in a bit more detail.

\paragraph{Adversarial Loss} is already used in training of standard \ac{GAN}s. It arises from the fact that the generator \textit{G} and discriminator \textit{D} work against each other. If the generator generates particularly good fake images that the discriminator can no longer identify as such, the loss of the discriminator automatically increases. On the other hand, the loss of the generator decreases. The loss behavior is similar the other way round. This is described mathematically in \cite{Source-GAN} as follows:
\begin{equation*}
\begin{split}
\min_{G} \max_{D} V(D,G) =&~\mathbb E_{x \sim p_{data}(x)} [\log D(x)] \\\
&+ \mathbb E_{z \sim p_{z}(z)} [\log (1-D(G(z)))]
\end{split}
\end{equation*}

\paragraph{Cycle Consistency Loss} is used additionally to the adversarial loss in the Cycle-\ac{GAN}. The idea has already been briefly explained before, that a generator \textit{G} translates an image \textit{Input\_ A} from the domain \textit{X} into an image \textit{B} of the domain \textit{Y}. In the backward translation of generator \textit{F}, an image \textit{Cyclic\_ A} from domain \textit{X} will be created from the image \textit{B} of domain \textit{Y}. The difference between image \textit{Input\_ A} and \textit{Cyclic\_ A} forms the cycle consistency loss \cite{Introduction-to-Cycle-GANs}. In \cite{Introduction-to-Cycle-GANs}, the cycle consistency loss is described mathematically as follows:
\begin{equation*}
	Loss_{cyc}(G,F,X,Y) = \frac{1}{m} \sum^{m}_{i=1}[F(G(x_i))-x_i]+[G(F(y_i))-y_i]
\end{equation*}

In summary, the Cycle-\ac{GAN} is built from two generators, \textit{Generator A2B} and \textit{Generator B2A}, and two discriminators, \textit{Discriminator A} und \textit{Discriminator B}. This architecture is shown simplified in \autoref{fig:simplified-cycle-gan}.

\begin{figure}[htb] 
	\centering 
	\includegraphics[width=\linewidth]{simplified-cycle-gan}
	\caption{A simplified architecture of a Cycle-\ac{GAN} \cite{Introduction-to-Cycle-GANs}}
	\label{fig:simplified-cycle-gan}
\end{figure}

\paragraph{Identity Loss} is also introduced in the paper \cite{image-to-image-ccan}. The loss is a regularization technic for the generator, to provide a better performs if the input is already near to the predicted output image. Mathematically the identity loss is defined in the paper \cite{image-to-image-ccan} as follows:
\begin{equation*}
\begin{split}
	Loss_{identity}(G,F) =&~\mathbb E_{y \sim p_{data}(y)}[||G(y) - y||_1] \\\
&+ \mathbb E_{x \sim p_{data}(x)}[||F(x)-x||_1]
\end{split}
\end{equation*}


\subsection{Datasets}
In addition to the architecture, a suitable datasets also had to be selected. On the one hand, it should be versatile, but on the other hand, it must not contain too large images so that the computing time is not too high. 

\paragraph{Tensorflow} provides different dataset specifically for Cycle-\ac{GAN}s including apple and orange images as well as photo and monet images. These ones are called \textit{cycle\_gan/apple2orange} and \textit{cycle\_gan/monet2photo}. The datasets contain two domains A and B each for training and test data. \autoref{tab:dataset} shows the four parts of the default configuration with the number of examples included in each dataset. Each example consist of image-label pairs.~\cite{google-tf-datasets}

\begin{table}[htb]
\centering
\caption{Structure of the standard configuration split of the Tensorflow dataset \textit{cycle\_gan/apple2orange} and \textit{cycle\_gan/monet2photo}~\cite{google-tf-datasets}}
\label{tab:datasetTF}
\begin{tabular}{c c c}
\textbf{index} & \makecell[cc]{\textbf{examples} \\ \textbf{\texttt{apple2orange}}} & \makecell[cc]{\textbf{examples} \\ \textbf{\texttt{monet2photo}}}\\ \hline
'testA' & 266 & 121 \\ \hline
'testB' & 248 & 751 \\ \hline
'trainA' & 995 & 1072 \\ \hline
'trainB' & 1019 & 6287 \\ \hline
\end{tabular}
\end{table}

\paragraph{Kaggle} provides another dataset \textit{arnaud58/selfie2anime}. In order to make a further comparison, another dataset from a different library was deliberately selected. This one contains images of anime and images of women selfies. The examples included in this dataset are shown in \autoref{tab:datasetKaggle}. \cite{kaggle-dataset}

\begin{table}[htb]
\centering
\caption{Structure of the standard configuration split of the Kaggle dataset \textit{arnaud58/selfie2anime}~\cite{kaggle-dataset}}
\label{tab:datasetKaggle}
\begin{tabular}{c c}
\textbf{index} & \makecell{\textbf{examples} \\ \textbf{\texttt{selfie2anime}}} \\ \hline
'testA' & 100 \\ \hline
'testB' & 100 \\ \hline
'trainA' & 3400 \\ \hline
'trainB' & 3400 \\ \hline
\end{tabular}
\end{table}
%------------------------------------------------
\section{Implementation}
The implementation is performed using the learned three steps, pre-processing, training and test from the module \ac{IANNWTF} \cite{implementingANsCourseware02, implementingANsCourseware03}. In addition, the methods mentioned in the section before are used to develop the Cycle-\ac{GAN} architecture. Furthermore, other functions of the Tensorflow and Keras libraries are used in all parts of the implementation. Therefore, \texttt{tensorflow}, \texttt{tensorflow\_datasets}, \texttt{tensor- flow\_addons} and \texttt{matplotlib.pyplot} are imported. How the implementation is realized and the different libaries are used, is now explained in more detailed.

\subsection{Network Architecture}
The architecture is built as explained in section methods before. A class \texttt{Cycle\_GAN\_Generator} and a class \texttt{Cycle\_ GAN\_Discriminator} are programmed. Two instances of each of the two classes are then created, the \texttt{generator\_A \_to\_B} and \texttt{generator\_B\_to\_A}, and the \texttt{discriminator \_A} and \texttt{discriminator\_B}.

\paragraph{Generator Class} inherits from the Keras class \texttt{Layer}. The class in general is build up of three parts, the \textit{Encoder}, \textit{Transformer} and \textit{Decoder}. Each part consists again out of different layers. The structure of the generator is shown in \autoref{fig:generator}.~\cite{Introduction-to-Cycle-GANs}

\begin{figure*}[htb] 
	\centering 
	\includegraphics[width=\linewidth]{generator}
	\caption{The high-level structure of the Cycle-\ac{GAN}s generator \cite{Introduction-to-Cycle-GANs}}
	\label{fig:generator}
\end{figure*}

The \textbf{Encoder} extracts the feature of the original input image, for which convolutional layers are normally used \cite{Introduction-to-Cycle-GANs}. In \texttt{Cycle\_GAN\_Generator}, the encoder also consists of exactly three convolutional layers, as also described in \autoref{fig:generator}. The Keras class \texttt{Conv2D} is used for this. As activation function \ac{ReLu} is selected, for which the Keras class \texttt{ReLu} of the same name is used. Furthermore, the Keras class \texttt{InstanceNormalization} is used, which is a special form of group normalization, see \cite{google-tf-InstanceNormalization}, and is therefore suitable for the dataset used for the reimplementation. So that the training does not exceed the time frame of the project, only a quarter of the original filter sizes of the paper are used in each \texttt{Conv2D} layer. All other values for \texttt{kernel\_size}, \texttt{strides}, \texttt{activation}, \texttt{padding} and the \texttt{kernel\_initializer} are taken identically from the specifications of the paper. The specifications and values for the three \texttt{Conv2D} layers are shown in \autoref{tab:encoderValues}.~\cite{image-to-image-ccan}

\begin{table}[htb]
\centering
\caption{Values of all convolutional layer specifications in the encoder part of the class \texttt{Cycle\_GAN\_Generator}~\cite{image-to-image-ccan}.}
\label{tab:encoderValues}
\begin{tabular}{c c c c}
\textbf{variable} & \makecell[cc]{\textbf{\texttt{Conv2D}} \\ \textbf{\texttt{Layer 1}}} & \makecell[cc]{\textbf{\texttt{Conv2D}} \\ \textbf{\texttt{Layer 2}}} & \makecell[cc]{\textbf{\texttt{Conv2D}} \\ \textbf{\texttt{Layer 3}}} \\ \hline
\textbf{\texttt{filters}} & 16 & 32 & 64 \\ \hline
\textbf{\texttt{kernel\_size}} & (7,7) & (3,3) & 3,3) \\ \hline
\textbf{\texttt{strides}} & (1,1) &  (2,2) & (2,2) \\ \hline
\textbf{\texttt{activation}} & None & None & None \\ \hline 
\textbf{\texttt{padding}} & "same" & "same" & "same" \\ \hline
\makecell[cc]{\textbf{\texttt{kernel\_}} \\ \textbf{\texttt{initializer}}} & \makecell[cc]{\texttt{Random} \\ \texttt{Normal}} & \makecell[cc]{\texttt{Random} \\ \texttt{Normal}} & \makecell[cc]{\texttt{Random} \\ \texttt{Normal}} \\ \hline
\end{tabular}
\end{table}




The \textbf{Transformer} is needed to relate closely spaced features extracted from the input image by the previous encoder \cite{Introduction-to-Cycle-GANs}. And this is exactly what the blocks of \acp{ResNet}, like shown in \autoref{fig:generator}, are needed for. Every \ac{ResNet} is build up by two \texttt{Conv2D} layers, two layers of \texttt{InstanceNormalization}, a \ac{ReLu} activation layer and a layer \texttt{Concatenate}, which is also a Keras class. A new class \texttt{ResNet\_Block}, which also inherits from the Keras class \texttt{Layer}, is implemented. All values for all layers are taken from the paper. The convolutional layer specifications are shown in \autoref{tab:transformerValues}.

\begin{table}[htb]
\centering
\caption{Values of the convolutional layer specifications in the transformer part of each \ac{ResNet} block of the class \texttt{ResNet\_Block}~\cite{image-to-image-ccan}.}
\label{tab:transformerValues}
\begin{tabular}{c c c}
\textbf{variable} & \makecell[cc]{\textbf{\texttt{Conv2D}} \\ \textbf{\texttt{Layer 1}}} & \makecell[cc]{\textbf{\texttt{Conv2D}} \\ \textbf{\texttt{Layer 2}}} \\ \hline
\textbf{\texttt{filters}} & 256 & 256 \\ \hline
\textbf{\texttt{kernel\_size}} & (3,3) & (3,3) \\ \hline
\textbf{\texttt{strides}} & (1,1) &  (1,1) \\ \hline
\textbf{\texttt{activation}} & None & None \\ \hline 
\textbf{\texttt{padding}} & "same" & "same" \\ \hline
\makecell[cc]{\textbf{\texttt{kernel\_}} \\ \textbf{\texttt{initializer}}} & \makecell[cc]{\texttt{Random} \\ \texttt{Normal}} & \makecell[cc]{\texttt{Random} \\ \texttt{Normal}} \\ \hline
\end{tabular}
\end{table}

Instances of this class are created as a layer in the class \texttt{Cycle\_GAN\_Generator}. The number of instances depends on the instance variable \texttt{n\_resnet}, which is set to six by default. Six \ac{ResNet} blocks are used if the image size is \textit{128~x~128}, nine blocks are use for an image size of \textit{256~x~256} or higher~\cite{image-to-image-ccan}.

The \textbf{Decoder} now leads back to an output image. For this, the low-level features are worked out with the help of the transpose convolution, see \cite{Introduction-to-Cycle-GANs}. To achieve this, the Keras class \texttt{Conv2DTranspose} is used in this part. Finally, as shown in \autoref{fig:generator}, another \texttt{Conv2D} is used, in conjunction with an activation function of the Keras class \texttt{tanh}. The values of the convolution layer specifications is found in \autoref{tab:decoderValues}.

\begin{table}[htb]
\centering
\caption{Values of the convolutional layer specifications in the transformer part of the class \texttt{Cycle\_GAN\_Genera-tor}~\cite{image-to-image-ccan}.}
\label{tab:decoderValues}
\begin{tabular}{c c c c}
\textbf{variable} & \makecell[cc]{\textbf{\texttt{Conv2D}} \\ \textbf{\texttt{Transpose}} \\ \textbf{\texttt{Layer 1}}} & \makecell[cc]{\textbf{\texttt{Conv2D}} \\ \textbf{\texttt{Transpose}} \\ \textbf{\texttt{Layer 2}}} & \makecell[cc]{\textbf{\texttt{Conv2D}} \\ \textbf{\texttt{Layer 4}}} \\ \hline
\textbf{\texttt{filters}} & 32 & 16 & 3 \\ \hline
\textbf{\texttt{kernel\_size}} & (3,3) & (3,3) & (7,7) \\ \hline
\textbf{\texttt{strides}} & (2,2) & (2,2) & (1,1) \\ \hline
\textbf{\texttt{activation}} & None & None & None \\ \hline 
\textbf{\texttt{padding}} & "same" & "same" & "same" \\ \hline
\makecell[cc]{\textbf{\texttt{kernel\_}} \\ \textbf{\texttt{initializer}}} & \makecell[cc]{\texttt{Random} \\ \texttt{Normal}} & \makecell[cc]{\texttt{Random} \\ \texttt{Normal}} & \makecell[cc]{\texttt{Random} \\ \texttt{Normal}} \\ \hline
\end{tabular}
\end{table}


\paragraph{Discriminator Class} also inherits from the Keras class \texttt{Layer}. In this reimplementation the class is just simple network, including different layers. Since it has an image as input and a simple decision vector as output, \texttt{Conv2D} layers, are associated with \texttt{InstanceNormalization} and different activation functions such as \texttt{ReLu} and \texttt{LeakyReLu}, see following pseudocode. The structure of the discriminator is shown in \autoref{fig:discriminator}. \cite{Introduction-to-Cycle-GANs}

\begin{figure*}[htb] 
	\centering 
	\includegraphics[width=0.8\linewidth]{discriminator}
	\caption{The high-level structure of the Cycle-\ac{GAN}s discriminator \cite{Introduction-to-Cycle-GANs}}
	\label{fig:discriminator}
\end{figure*}

\begin{lstlisting}
self.conv_1 = tf.keras.layers.Conv2D(...)
??                                        
self.activ_1 = tf.keras.layers.LeakyReLU(...)

self.conv_2 = tf.keras.layers.Conv2D(...)
self.instance_norm_2 = tfa.layers.InstanceNormalization(...)                                     
self.activ_2 = tf.keras.layers.LeakyReLU(...)  

self.conv_3 = tf.keras.layers.Conv2D(...)
self.instance_norm_3 = tfa.layers.InstanceNormalization(...)                                     
self.activ_3 = tf.keras.layers.LeakyReLU(...)    

self.conv_4 = tf.keras.layers.Conv2D(...)
self.instance_norm_4 = tfa.layers.InstanceNormalization(...)                                     
self.activ_4 = tf.keras.layers.LeakyReLU(...)              

self.patch_output = tf.keras.layers.Conv2D(...)
\end{lstlisting}


\subsection{Pre-Processing}
In addition to creating the network architecture, the dataset must also be preprocessed. Since this is a Tensorflow dataset, it can be loaded more easily with the \texttt{tfds.load} function. When the dataset is loaded, it is also directly split into test and training data and requested as a 2-tuple structure (input, label). To ensure that all images are in the same format and that the training time is not too high, the images are subsequently resized to \textit{128x128}. The \texttt{image.resize} function of the Tensorflow library is used for this. After normalization and reshaping the tensor to \textit{128x128x3}, the dataset is shuffled and prefetched. Batching is omitted to facilitate the later definition of real samples in training. The pseudocode shows all pre-processing steps.
\begin{lstlisting}
train_data, test_data = tfds.load(...)

train_data = tf.image.resize each train_image to [128, 128]
train_data = 2 * (each train_image / 255) -1)
train_data = tf.reshape each train_image_tensor to [128, 128, 3]
train_data = train_data.shuffle(...).prefetch(...)

test_data = tf.image.resize each test_image to [128, 128]
test_data = 2 * (each test_image / 255) -1)
test_data = tf.reshape each test_image_tensor to [128, 128, 3]
test_data = test_data.shuffle(...).prefetch(...)
\end{lstlisting}

\begin{enumerate}[noitemsep] % [noitemsep] removes whitespace between the items for a compact look
	\item Warum shuffel 32?
	\item warum prefetch 32?
	\item Warum bilinear ge resized?
\end{enumerate}

\subsection{Training}
\begin{enumerate}[noitemsep] % [noitemsep] removes whitespace between the items for a compact look
	\item Welche Schritte gibt es im Training? Generator/Discriminator
	\item Welche Klassen/Funktionen wurden hierfür dann geschrieben?
	\item Wo sind relevanten Werte gewählt worden? Warum wurden sie so gewählt?(z.B. Numbers of epoch?)
\end{enumerate}

\subsection{Test}
\begin{enumerate}[noitemsep] % [noitemsep] removes whitespace between the items for a compact look
	\item Welche Schritte gibt es im Test? Generator/Discriminator, einfacher Forward Step?
	\item Ausgabe/Ergebnis anzeigen, wie umgesetzt?
\end{enumerate}

%------------------------------------------------
\section{Evaluation}
\begin{enumerate}[noitemsep] % [noitemsep] removes whitespace between the items for a compact look
	\item Wir haben improvements anhand folgender zwei paper vorgenommen
\end{enumerate}

\subsection{Improved Paper 1}

\subsection{Improved Paper 2}

%\begin{table}[hbt]
%	\caption{Table of Grades}
%	\centering
%	\begin{tabular}{llr}
%		\toprule
%		\multicolumn{2}{c}{Name} \\
%		\cmidrule(r){1-2}
%		First name & Last Name & Grade \\
%		\midrule
%		John & Doe & $7.5$ \\
%		Richard & Miles & $2$ \\
%		\bottomrule
%	\end{tabular}
%	\label{tab:label}
%\end{table}

%------------------------------------------------

\section{Results}

\begin{enumerate}[noitemsep] % [noitemsep] removes whitespace between the items for a compact look
	\item Welche Ergebnisse erzielt das Training?
	\item Welches Ergebnis erzielt der Test?
\end{enumerate}


%------------------------------------------------

\section{Limitations and Discussion}

\begin{enumerate}[noitemsep] % [noitemsep] removes whitespace between the items for a compact look
	\item Datensatz/Bildgröße
	\item Dauer des Trainings und beanspruchung der rechenzeit
\end{enumerate}
%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------

\phantomsection
\bibliographystyle{unsrt}
\bibliography{sample}

%----------------------------------------------------------------------------------------

\begin{acronym}
\acro{BAIR}[BAIR]{Berkeley AI Research}
\acro{IANNWTF}[IANNWTF]{Implementing Artificial Networks with Tensorflow}
\acro{GAN}[GAN]{Generative Adversarial Network}
\acro{ReLu}[ReLu]{Rectified Linear Units}
\acro{ResNet}[ResNet]{Residual Network}
\acroplural{ResNet}[ResNet]{Residual Networks}
%\acroplural{Kuerzel}[Kurzform des Plurals]{Langform des Plurals}
\end{acronym}

%----------------------------------------------------------------------------------------


\end{document}